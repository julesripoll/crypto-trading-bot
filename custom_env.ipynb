{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce fichier est de créer l'environnement fonctionnel le plus simple possible.\n",
    "https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import keras\n",
    "from stable_baselines3 import DQN\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution pour le passage en live : mettre en argument de la classe des fonctions d'achats correspondant à celles définies dans bot_binance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward():\n",
    "    #todo benji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, df, window_size=5, tc=20) :\n",
    "        self.index=window_size\n",
    "        self.window_size=window_size\n",
    "        self.state=df[self.index-self.window_size:self.index]\n",
    "        self.tc=tc\n",
    "        #self.state=tf.Tensor(df[self.index-self.window_size:self.index])\n",
    "        self.df=df\n",
    "        self.holding=False\n",
    "        self.action_space=Discrete(3)\n",
    "        self.observation_space=Box(low=0, high=100000, shape=(5,))\n",
    "        self.total_reward=[]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        reward=0\n",
    "        #action : provided by agent given the state\n",
    "        price=self.df[self.index]\n",
    "        prev_price=self.df[self.index-1]\n",
    "        if action==0 : #buy\n",
    "            #print('buy')\n",
    "            reward=-price-self.tc\n",
    "            self.holding=True\n",
    "        elif action==1 and self.holding==True : #hold\n",
    "            #print('hold')\n",
    "            reward=price-prev_price\n",
    "        elif action==2 and self.holding==True : #sell\n",
    "            #print('sell')\n",
    "            reward=price-self.tc\n",
    "        \n",
    "        self.index+=1\n",
    "        self.state=self.df[self.index-self.window_size:self.index]\n",
    "        \n",
    "        done=False\n",
    "        if self.index>400: #va définir la durée d'un épisode. A voir si il y a pas une mnière plus intelligente de faire\n",
    "            done=True\n",
    "        info={}\n",
    "\n",
    "        self.total_reward.append((np.sum(self.total_reward)+reward))\n",
    "        #print(len(self.total_reward))\n",
    "        self.state=tf.reshape(self.state, shape=(5,))\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.index=self.window_size\n",
    "        self.state=df[self.index-self.window_size:self.index]\n",
    "        self.total_reward=[]\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "        #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chargement des données d'entrainement. Obsolète avec le script d'extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices=pd.read_csv(\"~/Developer/Trading bots/Trading bot crypto/Data/HitBTC_ETHUSD_1h.csv\", sep=\",\")\n",
    "prices = prices.rename(columns={'Close': 'value'})\n",
    "df=prices['value']\n",
    "df=pd.DataFrame({'value':df.values})\n",
    "df=df['value']\n",
    "env=TradingEnv(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "agt=DQN('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -1.42e+03 |\n",
      "|    exploration_rate | 0.398     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 9069      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 1584      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 396      |\n",
      "|    ep_rew_mean      | -2.6e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 9353     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3168     |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -1.06e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 9423      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 4752      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -2.48e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 9473      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 6336      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -3.11e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 9501      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 7920      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -5.51e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 9499      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 9504      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -6.88e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 9417      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 11088     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -6.83e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 9427      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 12672     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -7.09e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 9420      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 14256     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -7.17e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 9428      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 15840     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -6.89e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 9441      |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total_timesteps  | 17424     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -6.32e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 9432      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 19008     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -6.23e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 9384      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 20592     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 396      |\n",
      "|    ep_rew_mean      | -6.4e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 9376     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 22176    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 396       |\n",
      "|    ep_rew_mean      | -6.18e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 9370      |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total_timesteps  | 23760     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1056daf10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agt.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc stable baselines : https://stable-baselines.readthedocs.io/en/master/modules/dqn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt.save('trading agent')\n",
    "#agt=DQN.load('trading agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cellule de test de l'environnement. Pourra éventuellement être utilisée pour le live trading également. Attention ça tourne à l'infini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/julesripoll/Developer/Trading bots/Trading bot crypto/new and clean/custom_env.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/julesripoll/Developer/Trading%20bots/Trading%20bot%20crypto/new%20and%20clean/custom_env.ipynb#ch0000010?line=0'>1</a>\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/julesripoll/Developer/Trading%20bots/Trading%20bot%20crypto/new%20and%20clean/custom_env.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/julesripoll/Developer/Trading%20bots/Trading%20bot%20crypto/new%20and%20clean/custom_env.ipynb#ch0000010?line=2'>3</a>\u001b[0m     action, _states \u001b[39m=\u001b[39m agt\u001b[39m.\u001b[39;49mpredict(obs, deterministic\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m#que fait predict? Comme le predict d'un NN? Aussi, quelle nuance entre obs et state?\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/julesripoll/Developer/Trading%20bots/Trading%20bot%20crypto/new%20and%20clean/custom_env.ipynb#ch0000010?line=3'>4</a>\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/julesripoll/Developer/Trading%20bots/Trading%20bot%20crypto/new%20and%20clean/custom_env.ipynb#ch0000010?line=4'>5</a>\u001b[0m     env\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py:242\u001b[0m, in \u001b[0;36mDQN.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py?line=239'>240</a>\u001b[0m         action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample())\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py?line=240'>241</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py?line=241'>242</a>\u001b[0m     action, state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py?line=242'>243</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action, state\n",
      "File \u001b[0;32m~/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py:338\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=334'>335</a>\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=336'>337</a>\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=337'>338</a>\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=338'>339</a>\u001b[0m \u001b[39m# Convert to numpy\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/common/policies.py?line=339'>340</a>\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py:178\u001b[0m, in \u001b[0;36mDQNPolicy._predict\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py?line=176'>177</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, obs: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py?line=177'>178</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_net\u001b[39m.\u001b[39;49m_predict(obs, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n",
      "File \u001b[0;32m~/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py:71\u001b[0m, in \u001b[0;36mQNetwork._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py?line=68'>69</a>\u001b[0m q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(observation)\n\u001b[1;32m     <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py?line=69'>70</a>\u001b[0m \u001b[39m# Greedy action\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py?line=70'>71</a>\u001b[0m action \u001b[39m=\u001b[39m q_values\u001b[39m.\u001b[39;49margmax(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='file:///Users/julesripoll/miniforge3/envs/rl/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py?line=71'>72</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = agt.predict(obs, deterministic=True) #que fait predict? Comme le predict d'un NN? Aussi, quelle nuance entre obs et state?\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pb on a que du hold ou sell ou buy ça alterne jamais lors d'un même episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(BaseFeaturesExtractor):\n",
    "    #todo jules"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb86918481814cb9b030e802e82c2efc8b471ebf3a13b8fa0904159807e83a8f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('trading')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
