{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julesripoll/crypto-trading-bot/blob/jules_branch/bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_agents\n",
        "!pip install fastquant"
      ],
      "metadata": {
        "id": "u2cRiMuLaYjW",
        "outputId": "2e2df92e-e381-4f8f-9da5-bbcdd69a38a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tf_agents in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (0.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf_agents) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.15.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (2.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (4.2.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (0.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf_agents) (1.14.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf_agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf_agents) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf_agents) (0.16.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf_agents) (0.5.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.16.0->tf_agents) (0.1.7)\n",
            "Requirement already satisfied: fastquant in /usr/local/lib/python3.7/dist-packages (0.1.8.0)\n",
            "Requirement already satisfied: certifi>=2019.11.28 in /usr/local/lib/python3.7/dist-packages (from fastquant) (2021.10.8)\n",
            "Requirement already satisfied: pre-commit in /usr/local/lib/python3.7/dist-packages (from fastquant) (2.19.0)\n",
            "Requirement already satisfied: pytz>=2019.3 in /usr/local/lib/python3.7/dist-packages (from fastquant) (2022.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.8.2 in /usr/local/lib/python3.7/dist-packages (from fastquant) (4.11.1)\n",
            "Requirement already satisfied: lxml>=4.6.2 in /usr/local/lib/python3.7/dist-packages (from fastquant) (4.8.0)\n",
            "Requirement already satisfied: urllib3>=1.25.7 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.26.9)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from fastquant) (2.8.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from fastquant) (2.10)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from fastquant) (3.2.0)\n",
            "Requirement already satisfied: yfinance>=0.1.54 in /usr/local/lib/python3.7/dist-packages (from fastquant) (0.1.70)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from fastquant) (3.2.2)\n",
            "Requirement already satisfied: ccxt>=1.31.1 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.82.79)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from fastquant) (4.64.0)\n",
            "Requirement already satisfied: soupsieve>=1.9.5 in /usr/local/lib/python3.7/dist-packages (from fastquant) (2.3.2.post1)\n",
            "Requirement already satisfied: bs4>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from fastquant) (0.0.1)\n",
            "Requirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.7/dist-packages (from fastquant) (3.7)\n",
            "Requirement already satisfied: black>=19.10b0 in /usr/local/lib/python3.7/dist-packages (from fastquant) (22.3.0)\n",
            "Requirement already satisfied: backtrader>=1.9.75.123 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.9.76.123)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from fastquant) (2.6.3)\n",
            "Requirement already satisfied: PySocks>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.7.1)\n",
            "Requirement already satisfied: croniter>=0.3.35 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.3.5)\n",
            "Requirement already satisfied: tweepy>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from fastquant) (3.10.0)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from fastquant) (3.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.4.1)\n",
            "Requirement already satisfied: requests-oauthlib>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from fastquant) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from fastquant) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->fastquant) (4.2.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->fastquant) (0.9.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->fastquant) (0.4.3)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->fastquant) (2.5.2)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->fastquant) (1.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->fastquant) (8.1.3)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black>=19.10b0->fastquant) (2.0.1)\n",
            "Requirement already satisfied: yarl==1.7.2 in /usr/local/lib/python3.7/dist-packages (from ccxt>=1.31.1->fastquant) (1.7.2)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from ccxt>=1.31.1->fastquant) (3.0.0)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from ccxt>=1.31.1->fastquant) (37.0.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.7/dist-packages (from ccxt>=1.31.1->fastquant) (62.3.1)\n",
            "Requirement already satisfied: aiohttp>=3.8 in /usr/local/lib/python3.7/dist-packages (from ccxt>=1.31.1->fastquant) (3.8.1)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl==1.7.2->ccxt>=1.31.1->fastquant) (6.0.2)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from aiodns>=1.1.1->ccxt>=1.31.1->fastquant) (4.1.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.31.1->fastquant) (21.4.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.31.1->fastquant) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.31.1->fastquant) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.31.1->fastquant) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.31.1->fastquant) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.8->ccxt>=1.31.1->fastquant) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black>=19.10b0->fastquant) (4.11.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.6.1->ccxt>=1.31.1->fastquant) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt>=1.31.1->fastquant) (2.21)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->fastquant) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->fastquant) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->fastquant) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->fastquant) (2022.4.24)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.5->fastquant) (1.1.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance>=0.1.54->fastquant) (0.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black>=19.10b0->fastquant) (3.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.0.8 in /usr/local/lib/python3.7/dist-packages (from pre-commit->fastquant) (20.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from pre-commit->fastquant) (6.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pre-commit->fastquant) (0.10.2)\n",
            "Requirement already satisfied: identify>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pre-commit->fastquant) (2.5.0)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pre-commit->fastquant) (3.3.1)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from pre-commit->fastquant) (1.6.0)\n",
            "Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit->fastquant) (3.6.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit->fastquant) (0.3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6n26ico21p5s"
      },
      "outputs": [],
      "source": [
        "import tf_agents\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "\n",
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, df, tc=10, window_size=5, balance=1000,live=False):\n",
        "        self._action_spec= array_spec.BoundedArraySpec(shape=(), dtype=int, minimum=0, maximum=2)\n",
        "        self._observation_spec= array_spec.BoundedArraySpec(shape=(5,), dtype=float, minimum=0)\n",
        "        self._episode_ended=False\n",
        "        self.initial_balance=balance \n",
        "        self.current_balance=balance\n",
        "        self.train_df=df\n",
        "        self.train_index=window_size\n",
        "        self.tc=tc\n",
        "        self.window_size=window_size\n",
        "        self.state= df[self.train_index-window_size:self.train_index]\n",
        "        self.positioned=False\n",
        "        self.buying_price=0\n",
        "        if live==False:\n",
        "            self.len_train=df.shape[0]-1\n",
        "\n",
        "\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self.train_index=self.window_size\n",
        "        self.current_balance=self.initial_balance\n",
        "        self.state= self.train_df[self.train_index-self.window_size:self.train_index]\n",
        "        print('je vais être reset')\n",
        "        return ts.restart(observation=self.state)\n",
        "\n",
        "    def reward(self, ind, buying_price):\n",
        "        return self.train_df[ind]-buying_price-self.tc\n",
        "    \n",
        "    def _step(self, action):\n",
        "        \"\"\"\n",
        "        step retourne 4 valeurs : \n",
        "        -observation : une observation de l'environnement, ici une séquence de prix. object\n",
        "        -reward : une récompense, définie ci dessous, dépendant de l'action effectuée. float\n",
        "        -done : boolean indiquant si on doit reset l'env, terminant donc l'episode. boolean\n",
        "        -info : information de notre choix. dict \n",
        "\n",
        "        note : les actions possibles dépendent de si on est positionné ou non, et là c'est pas pris en compte.\n",
        "        \"\"\"\n",
        "        rwd=0\n",
        "        current_price=self.train_df[self.train_index]\n",
        "        if action==0 : #buy\n",
        "            rwd=0\n",
        "            self.buying_price=current_price\n",
        "            self.current_balance-=current_price\n",
        "            self.positioned=True\n",
        "        elif action==1: #do nothing\n",
        "            rwd=0\n",
        "        elif action==2: #sell\n",
        "            self.positioned=False\n",
        "            rwd=self.reward(self.train_index,self.buying_price)\n",
        "            self.current_balance+=current_price\n",
        "\n",
        "        self.train_index+=1\n",
        "        self.state=self.train_df[self.train_index-self.window_size:self.train_index]\n",
        "\n",
        "        if (self.train_index==self.len_train) or self.current_balance<self.initial_balance*0.9:\n",
        "            self.train_index=self.window_size\n",
        "            print(\"fini\")\n",
        "            step=ts.termination(reward=rwd, observation=self.state)\n",
        "            print(step.step_type) #si c'est bien 2 c'est ISLAST\n",
        "            return step\n",
        "            \"\"\"\n",
        "            ts.termination retourne un TimeStep. si termination à priori ça renvoie un done = true.\n",
        "            A verifier dans la cellule de validation. \n",
        "            Ici la condition d'arrêt c'est d'avoir perdu 10% de l'investissement initial ou d'avoir fini le training set\n",
        "            \"\"\"\n",
        "        else:\n",
        "            print(\"pas fini\")\n",
        "            step=ts.transition(reward=rwd, observation=self.state)\n",
        "            print(step.step_type) \n",
        "            return step  #s'il ne s'agit pas de la fin d'un épisode. Step type induit par transition\n",
        "\n",
        "    def _render(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q_o8DCuM1p5x"
      },
      "outputs": [],
      "source": [
        "#from trade import TradingEnv\n",
        "from tf_agents.agents import DqnAgent\n",
        "import pandas as pd\n",
        "from tf_agents.environments import tf_py_environment\n",
        "import tensorflow as tf\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.networks.q_rnn_network import QRnnNetwork\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import  TFUniformReplayBuffer\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.drivers.dynamic_episode_driver import DynamicEpisodeDriver\n",
        "from numpy import random\n",
        "from tf_agents.metrics import tf_metrics\n",
        "import numpy as np\n",
        "import tf_agents.trajectories as trajectory\n",
        "import fastquant as fq\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cellule d'import des données\n",
        "start='2021-01-01'\n",
        "end='2022-01-01'\n",
        "\n",
        "eth=fq.get_crypto_data('ETH/USDT',start,end)\n",
        "eth\n",
        "\n",
        "df=pd.DataFrame(eth['close'])\n",
        "df=df['close']"
      ],
      "metadata": {
        "id": "5JFMtxj_Zsoy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4mSV4djE1p5y"
      },
      "outputs": [],
      "source": [
        "#hyperparamètres\n",
        "initial_collect_steps = 100\n",
        "batch_size=32\n",
        "max_length=100 #pareil que initial collect steps?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jgv1pAMx1p50",
        "outputId": "f2ed426c-a8eb-4e1f-fca0-cd1ac8fa104e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dt\n",
            "2021-01-01     728.91\n",
            "2021-01-02     774.56\n",
            "2021-01-03     978.28\n",
            "2021-01-04    1041.43\n",
            "2021-01-05    1099.56\n",
            "Name: close, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "#init de l'environnement\n",
        "train_py_env=TradingEnv(df[:300])\n",
        "print(train_py_env.state)\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hVi-R8Mi1p53",
        "outputId": "295884e9-760f-48b9-c692-57c02310e7fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je vais être reset\n",
            "fini\n",
            "2\n",
            "ep finished\n",
            "je vais être reset\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "fini\n",
            "2\n",
            "ep finished\n",
            "je vais être reset\n",
            "fini\n",
            "2\n",
            "ep finished\n",
            "je vais être reset\n",
            "fini\n",
            "2\n",
            "ep finished\n",
            "je vais être reset\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "fini\n",
            "2\n",
            "ep finished\n",
            "je vais être reset\n",
            "num_episodes: 5 num_steps: 60\n",
            "avg_length 12.0 avg_reward: -24.593998\n"
          ]
        }
      ],
      "source": [
        "time_step = train_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 5\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action=random.randint(0,3)\n",
        "    time_step = train_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  print(\"ep finished\")\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = train_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dCcbacq-1p54"
      },
      "outputs": [],
      "source": [
        "#init du réseau\n",
        "rnn_net=QRnnNetwork(\n",
        "    input_tensor_spec=train_env.observation_spec(),\n",
        "    action_spec=train_env.action_spec(),\n",
        "    lstm_size=(40,),\n",
        "    output_fc_layer_params=(20,)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Juue48pP1p54"
      },
      "outputs": [],
      "source": [
        "#init de l'agent\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "global_step=tf.Variable(0)\n",
        "\n",
        "agent=DqnAgent(\n",
        "    time_step_spec=train_env.time_step_spec(),\n",
        "    action_spec=train_env.action_spec(),\n",
        "    q_network=rnn_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=global_step\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TFO9sD0n1p55"
      },
      "outputs": [],
      "source": [
        "#some data collection policies\n",
        "collect_pol=agent.collect_policy\n",
        "\n",
        "rdm_policy = random_tf_policy.RandomTFPolicy(action_spec=train_env.action_spec(),\n",
        "                                            time_step_spec=train_env.time_step_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ctrl+M M pour cellule en texte"
      ],
      "metadata": {
        "id": "oz2YO1tifO3o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZX_6DJO1p56"
      },
      "source": [
        "en dessous on a plusieurs essais de driver et replay buffer. Voir la doc tensorflow (notebooks dqn agent with tf-agents, et replay buffers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Hw8Udmtr1p5_"
      },
      "outputs": [],
      "source": [
        "#ne marche pas\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=1000\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Yc2jl49H1p6A",
        "outputId": "6a153511-efb5-4b7b-a784-b9ac4d93f409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je vais être reset\n",
            "pas fini\n",
            "1\n",
            "pas fini\n",
            "1\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n",
            "fini\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "train_env.reset\n",
        "num_episodes = tf_metrics.NumberOfEpisodes()\n",
        "env_steps = tf_metrics.EnvironmentSteps()\n",
        "observers = [replay_buffer.add_batch,num_episodes, env_steps]\n",
        "replay_buffer.clear()\n",
        "driver = DynamicEpisodeDriver(\n",
        "    train_env, collect_pol, observers,num_episodes=10)\n",
        "\n",
        "# Initial driver.run will reset the environment and initialize the policy.\n",
        "final_time_step, policy_state = driver.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vBkH7ypr1p6C",
        "outputId": "1f2ade86-6fbb-4ae1-c4e2-4ac197a45aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ]
        }
      ],
      "source": [
        "# Read the replay buffer as a Dataset,\n",
        "# read batches of 4 elements, each with 2 timesteps:\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=4,\n",
        "    num_steps=2)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "\n",
        "num_train_steps = 10\n",
        "\n",
        "for _ in range(num_train_steps):\n",
        "  trajectories, _ = next(iterator)\n",
        "  loss = agent.train(experience=trajectories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ykx0SHeT1p6D"
      },
      "outputs": [],
      "source": [
        "#init de l'environnement\n",
        "train_py_env=TradingEnv(df[3000:])\n",
        "eval_env = tf_py_environment.TFPyEnvironment(train_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OYjccDT11p6D",
        "outputId": "835c8584-86cc-4b8d-aa9e-17e5674fd05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 5.72 µs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-988daf6dde0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Evaluate the agent's policy once before training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'compute_avg_return' is not defined"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdH5ATt41p6E"
      },
      "source": [
        "le problème est au niveau du calcul de la loss. Je crois que le 40 implique que je prends 40 trajectoires. Voir comment on compute une loss pour un rnn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e9k1DSb1p6F"
      },
      "outputs": [],
      "source": [
        "#train the agent\n",
        "dataset=replay_buffer.as_dataset(num_steps=40) #40 : taille de la séquence, correspond au nb de cellule du rnn, pour compute la loss\n",
        "for batched_experience in dataset:\n",
        "    print(batched_experience)\n",
        "    agent.train(batched_experience)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "34f818cfff9ef3400c9acfecabbb5fbd22e02c3f9d612971b6b631c436e47a91"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('rl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "bot.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}